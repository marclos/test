---
title: "Los Angeles County Covid-19 Analysis"
author: "Marc Los Huertos"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output: bookdown::html_document2
# output: github_document
---
   
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, options(scipen=999))
library(tidyverse)
library(lubridate)
require(deSolve)
require(bbmle)
```

# Introduction

As I have watched the Covid-19 pandemic, I decided that I was going to process the data on my own and see of I could confirm trends articulated by state and federal agencies. From what I can tell, each region has it's own timeline and trajectory based municipal responses and state mandates. Thus, I took some time to drill into the LA context. 

## Disclaimer

This is not peer reviewed work and well outside my typical professional work -- but these mussings are mostly to help me understand what's going on, but if others find this useful, please analyze the data for yourself to confirm my results. If you found something amiss with my analyses, please let me know and I'll fix it ASAP. 

# Data Source

I have been using the [data hosted on github](https://github.com/CSSEGISandData/COVID-19), entitled Novel Corona-virus (COVID-19) Cases and provided by Johns Hopkins University, Center for Systems Science and Engineering.

# Work flow

First, I create a list of daily reports and combined them into a single csv. Unfortunately, the folks curating the data have been terribly inconsistent in the with their data records as columns are added and inconsistently named. Unfortunately, reading the data into R took 5 hours because there were at least three different file structures, but should have taken 5 minutes. 

```{r, echo=FALSE, results='hide', message=FALSE}
# download and read csvs
#I am using project connect to the gibhub site and pull into my own project. I can't push, but that's fine. 

# create list of csv
library(readr)
library(dplyr)
path = "~/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/" 
#path = "https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports/"
filepath <- list.files(path = path, pattern = "*.csv", full.names = T);
files <- list.files(path = path, pattern = "*.csv", full.names = F);
files = substring(files, 1, 10)
files = data.frame(files, filepath)
Daily = data.frame(NA)
for(i in 1:c(nrow(files)-0)){
   tmp = paste(files[i,2])
   tmp2 <- read.csv(tmp)
   tmp2$Date = paste(files[i,1])
   tmp2[] <- lapply(tmp2, as.character)
Daily = dplyr::bind_rows(Daily, tmp2)
}

```

## Subset Southern California

I have only selected for Los Angeles so far, but the data is a mess because Los Angeles disappears for a week, so I had to make a decision about what to do about missing data -- at a key time! For two weeks the values seem to be missing. I will need to work on figuring out why. 

The numbers creep up slowly in February -- as we can see when we limit the y-axis values. 
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap ="Early Records of Confirmed Cases of Covid-19 in LA County"}
#str(Daily)
Daily$Confirmed=as.numeric(Daily$Confirmed)
Daily$Deaths[is.na(Daily$Deaths)]=0; Daily$Deaths <- as.numeric(Daily$Deaths)
Daily$lnConfirmed = log(Daily$Confirmed)
Daily$Recovered = as.numeric(Daily$Recovered)
Daily$Date= as.Date(as.character(Daily$Date), format='%m-%d-%Y')

str(Daily$Date)
#subset(Daily, FIPS==c(6037))
CA = subset(Daily, subset=(Province_State=="California"))

LA = subset(Daily, subset=(Admin2 == "Los Angeles" | Province.State == "Los Angeles, CA"))
str(LA)
Orange = subset(Daily, subset=(Admin2 == "Orange" & Province_State == "California" | Province.State == "Orange, CA"))
SB = subset(Daily, subset=(Admin2 == "San Bernardino" & Province_State == "California" | Province.State == "San Bernardino, CA"))
Riverside = subset(Daily, subset=(Admin2 == "Riverside" & Province_State == "California" | Province.State == "Riverside, CA"))
SD = subset(Daily, subset=(Admin2 == "San Diego" & Province_State == "California" | Province.State == "San Diego, CA"))

#LA$Deaths[is.na(LA$Deaths)]=0; LA$Deaths <- as.numeric(LA$Deaths)
#LA$Date= as.Date(as.character(LA$Date), format='%m-%d-%Y')
#SB$Date= as.Date(as.character(SB$Date), format='%m-%d-%Y')
#Riverside$Date= as.Date(as.character(Riverside$Date), format='%m-%d-%Y')
#Orange$Date= as.Date(as.character(Orange$Date), format='%m-%d-%Y')
#SD$Date= as.Date(as.character(SD$Date), format='%m-%d-%Y')

plot(Confirmed~Date, data=LA, las=1, ylim=c(0,18), ty='b')

```

And suddenly, we are missing data at the key period of time in the second week of March. I would hesitate to interpolate the missing data because as we'll see in the next section, how the slope is calculated has extreme effects on the double rate. 

```{r, echo=FALSE, fig.cap='Confirmed Cases in Los Angeles County (Source JHU-CSSE)'}

plot(Confirmed~Date, data=LA, las=1, pch=20)
points(Orange$Date, Orange$Confirmed, col="orange", cex=.7)
points(Riverside$Date, Riverside$Confirmed, col="blue", cex=.7)
points(SB$Date, SB$Confirmed, col="brown", cex=.7)
points(SD$Date, SD$Confirmed, col="purple", cex=.7)

legend(as.Date("2020-02-15"), 25000, legend=c("LA County", "Riverside", "Orange", "San Bernardino", "San Diego"), pch=20, 
  col=c("black", "blue", "orange", "brown", "purple"))
```

## Daily Cases, Deaths

```{r dailycases, eval=FALSE}
CA$New = NA; CA$Week = NA

# Subset counties - remove Modoc and Lassen
CA = subset(CA, subset=(Admin2!="Modoc" | Admin2!="Lassen"))
CA = CA[order(CA$Date),]
counties <- unique(CA$Admin2)[-59]

# Weekly Changes
for(i in 1:length(counties)){
  tmp = subset(CA, subset=(Admin2==counties[i]))$Confirmed
  new = sapply(1:length(tmp), function(x) tmp[x+1]) - tmp
  new = c(NA, new[-length(new)])
  week = round((1:length(new)+3.5)/7, 0)
  CA$New[CA$Admin2==counties[i]] = new
  CA$Week[CA$Admin2==counties[i]] = week
}

Slopes = data.frame(County=NA, Week=NA, Slope=NA)
#Daily$New <- sapply(1:nrow(Daily), function(x) Daily$Confirmed[x+1])
i = 5; j=10
for(i in 1:max(CA$Week, na.rm=T)){
  for(j in 1:length(counties)){
m1 = lm(New ~ Date, CA[CA$Week==i & CA$Admin2==counties[j],])

Slopes = rbind(Slopes, 
               data.frame(County=counties[j], Week=i, Slope= as.numeric(round(coef(m1)[2],2))))
                             
  }}

library(dplyr)
library(tidyr)
CA %>% 
  group_by(Admin2) %>% 
  do(lm(lnConfirmed~Date, data=.) %>% coef %>% as_tibble)

```


## Using LA County Public Health Data 

I found press releases by the County to have some of the missing data:

- 3/09/2020  = 16 cases (github reported 14)
- 3/10/2020  = 17 cases, but missing LB and Pasadena
- 3/11/2020  = 27 cases
- 3/12/2020  = 32 cases
- 3/13/2020  = 40 cases
- 3/14/2020  = 53 cases
- 3/15/2020  = 69 cases
- 3/16/2020  = 94 cases
- 3/17/2020  = 144 cases
- 3/18/2020  = 190 cases
- 3/19/2020  = 231 cases
- 3/20/2020  = 292 cases
- 3/21/2020  = 351 cases
- 3/22/2020  = 409 cases (corrected as 407 on the 23rd, same as github)
- 3/23/2020  = 536 cases (90 hospitalized) same as github

```{r countydata, echo=FALSE}
county = data.frame(Date = as.Date(c("2020-03-10", "2020-03-11", "2020-03-12", "2020-03-13", "2020-03-14", "2020-03-15", "2020-03-16", "2020-03-17", "2020-03-18", "2020-03-19", "2020-03-20", "2020-03-21")), Confirmed = c( 17, 27, 32, 40, 53, 69, 94, 144, 190, 231, 292, 351))
county$lnConfirmed = log(county$Confirmed)

LA = bind_rows(LA, county)
```

   
Missing or incorrect data on Github site from the weekend of the March 28/29, April 4, April 11/12 based on the County Health Department Webpage (noon on each day).
```{r}
LA$Confirmed[LA$Date=="2020-03-28"] = 1829
LA$Confirmed[LA$Date=="2020-03-29"] = 2136
LA$Confirmed[LA$Date=="2020-04-04"] = 5277
LA$Confirmed[LA$Date=="2020-04-11"] = 8873
LA$Confirmed[LA$Date=="2020-04-12"] = 9162
LA = LA[order(LA$Date),]
```
## Tracking of Recovered

The County has not reported recovery rates; However, for our purposes, I'll subtract mortality from confirmed cases to estimate the number of recovered and ofset the recover by 18 days from first first confirmed. It's far from ideal, but useful in the modeling.
```{r}

LA$Recovered = LA$Confirmed - LA$Deaths
LA$Recovered = lag(LA$Recovered, 18)
LA$Recovered[LA$Recoverved == 0] = NA; LA[sort(sample(1:nrow(LA), 5)),c(8,5:7)]

```


## Plotting Covid-19 in Los Angeles County

```{r, echo=FALSE, fig.cap='Confirmed Cases in Los Angeles County (Source JHU-CSSE and LA County Public Health)'}
par(mfrow=c(1,2))

plot(Confirmed~Date, data=LA, las=1, pch=20, col="red", 
     xlim=c(as.Date("2020-03-01"), as.Date("2020-03-20")),
     ylim=c(0,300))
points(Deaths~Date, data=LA[LA$Deaths>0,], pch=20, col="black")
points(Recovered~Date, data=LA[LA$Recovered>0,], pch=20, col="green")
legend(as.Date("2020-02-05"), 5500, legend=c("Confirmed Cases", "Recovered", "Deaths"), pch=20, col=c("red", "green", "black"))

plot(Confirmed~Date, data=LA, las=1, pch=20, col="red")
points(Deaths~Date, data=LA[LA$Deaths>0,], pch=20, col="black")
points(Recovered~Date, data=LA[LA$Recovered>0,], pch=20, col="green", cex=.6)
legend(as.Date("2020-02-05"), 28000, legend=c("Confirmed Cases", "Recovered", "Deaths"), pch=20, col=c("red", "green", "black"), cex=.8)
```



# Determine Doubling Rate

Doubling rate is an estimate of how many days (in this case) we have seen the confirmed cases double. 

The analysis is based on the slope of the natural log of the confirmed cases and the slope. 

But the slope depends on how the early part of the record is included, which is then also sensitive to the missing data -- which is why I manually entered the data from the County's advisories. 

In this case, I used both the complete data and the data that skips the first 32 observations. Notice how much better the second line (dark red) fits the more recent records compared to the purple line that includes all the data. 

One could argue that there was too much uncertainty even in the first week of March to include in the model, however, I think it's useful to include because it's sets a better stage in how policy makers must respond in spite of uncertainty. 

```{r, echo=FALSE, results='hide'}
#LA$lnConfirmed = log(LA$Confirmed)
LA.lm.bad = lm(lnConfirmed~Date, data=LA)
LA.lm.bad.sum = summary(LA.lm.bad)
LA.lm.bad.sum$r.squared 
LA.lm = lm(lnConfirmed~Date, data=LA[-c(1:32),])

LA.lm.sum = summary(LA.lm)
# names(LA.lm.sum)
dr.bad = as.vector(round(log(2)/LA.lm.bad$coef[2],1)); dr.bad
dr = as.vector(round(log(2)/LA.lm$coef[2],1)); dr

# Confidence Intervals
conf_int <- predict(LA.lm,LA, interval="confidence")
LA = cbind(LA, conf_int); head(LA)
```
As it turns out the first 31 days there is only one confirmed infection -- so, it's better to start when we have an actual change, and on day 32, there were 7 infections. 

Using the r^2^ to evaluate the models, we find the model that uses all the data has a r^2^ of `r round(LA.lm.bad.sum$r.squared, 3)`, while the regression that excludes the early part of the dataset has an r^2^ of `r round(LA.lm.sum$r.squared, 3)`. The closer to one is the better and these results reinforce my decision to exclude the early part of the dataset. 

If we include the entire data set, the doubling rate is `r dr.bad`. Meanwhile, the doubling rate for the data that excludes the early part of the record is `r dr`, which is probably more consistent with the pattern of this epidemic. 

```{r, echo=FALSE, fig.cap ='Confirm Cases in Los Angeles County and Estimated Doubling Rates'}

plot(lnConfirmed ~ Date, data=LA, las=1, ylab="ln(Confirmed)")
abline(coef(LA.lm), col='darkred', lwd=1.4)
abline(coef(LA.lm.bad), col='purple')
text(as.Date("2020-02-16"), 1.8,paste("Doubling Rate =", dr.bad), col='purple')
text(as.Date("2020-03-05"), 5.1, paste("Doubling Rate =", dr), col='darkred')
```


# Policy Lag Times

Seems to me that we are about two weeks behind in making decisions to protect public health. The Governor's order to stay home was made on the March 19th, thus we should see the same trend in growth until the effects of the order slow the spread of the virus, which should be about the 3rd of April, when we should see a decline of the doubling rate. 

```{r echo=FALSE, results='hide', fig.cap='Predicted Number of Covid-19 Cases in Los Angeles', warning=FALSE, message=FALSE}

plot(lnConfirmed ~ Date, data=LA, las=1, xlim=c(as.Date('2020-02-01'), as.Date('2020-05-05')), pch=20,
ylim=c(0,13.5), ylab="ln(Confirmed)")
lines(LA$Date, LA$fit, col='darkred', lwd=1.4)
lines(LA$Date, LA$upr, col='darkorange', lwd=1, lty=2)
lines(LA$Date, LA$lwr, col='darkorange', lwd=1, lty=2)

#new_dates <- tibble(Date=ymd(c('2020-04-03', '2020-04-09', '2020-04-12', '2020-04-15', '2020-04-18','2020-04-21', '2020-04-24', '2020-04-27')))

temp = as.Date(c(Sys.Date()+seq(0, 21, 3)))

new_dates=tibble(Date= ymd('2020-04-03', temp))
  
pred_vals <- predict(LA.lm, new_dates, interval="predict")
prediction =cbind(new_dates, pred_vals); prediction

lines(prediction$Date, prediction$fit, col='red', lwd=1.4)
lines(prediction$Date, prediction$upr, col='red', lwd=1, lty=2)
lines(prediction$Date, prediction$lwr, col='red', lwd=1, lty=2)

legend(as.Date('2020-02-01'), 6, legend=c("Best Fit Line", "Confidence Intervals", "Prediction", "Prediction Intervals"), lwd=1.4, col=c("darkred", "darkorange", "red", "red"), lty=c(1,2,1,2), cex=.6, bty="n") 


abline(h=prediction[1, 2], col="darkgreen")
abline(h=prediction[3, 2], col="blue")
abline(h=prediction[5, 2], col="purple")

new_dates2 = as.Date(new_dates$Date, origin = "1970-01-01"); new_dates2
new_dates3 = format(new_dates2,"%m/%d"); new_dates3

#text(as.Date('2020-03-01'), prediction[1, 2], paste("Confirmed =", round(exp(prediction[1, 2]),-1)), col='darkgreen', pos=1) 

text(as.Date('2020-03-01'), prediction[3, 2], paste("Predicted for (", new_dates3[3],") =", round(exp(prediction[3, 2]),-1)), col='blue', pos=1)

text(as.Date('2020-03-01'), prediction[5, 2], paste("Predicted for (", new_dates3[4],") =", round(exp(prediction[5, 2]),-1)), col='purple', pos=1) 

rect(as.Date('2020-03-19'),5, as.Date('2020-04-03'), prediction[1, 2], border=T)

```

Based on the doubling rates, I had predicted the following number of confirmed cases: 

 - March 25th 6170
 - March 26th 6620
 - March 27th 6810
 - March 28th 7230
 - March 29th 7190
 - March 30th 7040
 - March 31st 6890
 - April 3rd 6180
 
Thank fully, on the 3rd of April we only had 5277, much lower than I had predicted -- thank goodness, and seems to suggest that social distancing is working. 

Now, just two more weeks and the county may be largely free of the disease and another month the country should be able to get back to some normalcy. But testing is going to have to increase dramatically. 
 
*Weekend values are not updated on github site for LA County. Thus, I have used LA County health updates to fill in on weekends. 

## Testing for an Nonlinear Growth Rate

In theory the US travel and shelter-at-home restrictions should reduce the rate so spread. I wonder when we'll start to see that signal?  In other words, we do the values fall outside the expected range -- i.e. when is there evidence that the exponential growth rate flattening?  

```{r testingnewvalues, echo=FALSE, results='hide'}
LA.test = LA[-c(1:32),]; LA.test
LA.test = LA.test[LA.test$Date<="2020-04-01",];

LA.lm2 = lm(lnConfirmed~Date, data=LA.test)

test = data.frame(Date = max(LA.test$Date), Confirmed=LA$Confirmed[LA$Date=="2020-04-01"]); test

tested <- predict(LA.lm2, test, interval="confidence");

result = cbind(test, logConfirmed = log(test$Confirmed), lwr = tested[,2], upr = tested[,3]); result
#LA = cbind(LA, conf_int); head(LA)

```

With this simple analysis, I obtained the following results: 

```{r, echo=FALSE}
print(result)
```


Based on this, the first confident indication that the curve was flattening was on the 1st of April -- but you can see the trend visually before that!

## Comparing Doubling Rate Changes by SoCal Counties

The doubling rate, i.e. the number of days before the number of confirmed cases, is a good measure of the near term changes we can expect for Covid-19. How quickly the doubling rate might decrease as a function of municipal and state recommendations and orders is quite difficult to estimate. It depends on 'shelter-in-place' compliance, the number of people getting the virus and the testing rate. I suspect it will be uneven, temporally and spatially.  

```{r, echo=FALSE, results='hide'}

# LA Dates
LA1.dates = seq(as.Date("2020-03-03"), as.Date("2020-03-28"), 1)
LA2.dates = seq(as.Date("2020-03-27"), as.Date("2020-04-10"), 1); #LA2.dates
LA3.dates = seq(as.Date("2020-04-09"), max(LA$Date)+1, 1); LA3.dates

LA1 = subset(LA, subset=(Date >= min(LA1.dates) - 1 & Date <= max(LA1.dates)-1)); nrow(LA1); # LA1$Date
LA2 = subset(LA, subset=(Date >= min(LA2.dates) - 1 & Date <= max(LA2.dates)-1)); # LA2$Date
LA3 = subset(LA, subset=Date >= min(LA3.dates)+1); LA3$Date

# Adding Orange County
Orange1.dates = seq(as.Date("2020-03-21"), as.Date("2020-03-30"), 1)
Orange2.dates = seq(as.Date("2020-03-29"), as.Date("2020-04-10"), 1) 
Orange3.dates = seq(as.Date("2020-04-09"), max(Orange$Date)+1, 1)

Orange1 = subset(Orange, subset=(Date>=min(Orange1.dates) - 1 &  Date<=max(Orange1.dates)-1)); nrow(Orange1); #Orange1$Date
Orange2 = subset(Orange, subset=(Date>=min(Orange2.dates) - 1 &  Date<=max(Orange2.dates)-1)); nrow(Orange2); #Orange2$Date
Orange3 = subset(Orange, subset=Date>=min(Orange3.dates)+1); Orange3$Date

LA1.lm = lm(lnConfirmed~Date, data=LA1)
LA2.lm = lm(lnConfirmed~Date, data=LA2)
LA3.lm = lm(lnConfirmed~Date, data=LA3)
LA3.dr = as.vector(round(log(2)/LA3.lm$coef[2],1)); LA3.dr

Orange1.lm = lm(lnConfirmed~Date, data=Orange1)
Orange2.lm = lm(lnConfirmed~Date, data=Orange2)
Orange3.lm = lm(lnConfirmed~Date, data=Orange3)
Orange3.dr = as.vector(round(log(2)/Orange3.lm$coef[2],1)); Orange3.dr

LA1.pred <- data.frame(Date = as.Date(LA1.dates), predict(LA1.lm, tibble(Date = ymd(LA1.dates)), interval="predict"))
LA2.pred <- data.frame(Date = as.Date(LA2.dates), predict(LA2.lm, tibble(Date = ymd(LA2.dates)), interval="predict"))
LA3.pred <- data.frame(Date = as.Date(LA3.dates), predict(LA3.lm, tibble(Date = ymd(LA3.dates)), interval="predict"))

Orange1.pred <- data.frame(Date = as.Date(Orange1.dates), predict(Orange1.lm, tibble(Date = ymd(Orange1.dates)), interval="predict")); #str(Orange1.pred)
Orange2.pred <- data.frame(Date = as.Date(Orange2.dates), predict(Orange2.lm, tibble(Date = ymd(Orange2.dates)), interval="predict")); #str(Orange2.pred)
Orange3.pred <- data.frame(Date = as.Date(Orange3.dates), predict(Orange3.lm, tibble(Date = ymd(Orange3.dates)), interval="predict")); str(Orange3.pred)
```


```{r countydoubleratecompare, echo=FALSE}
plot(lnConfirmed ~ Date, data=LA, las=1, ylab="ln(Confirmed)",
     xlim=c(as.Date("2020-03-03"), max(LA$Date)))
points(Orange$Date, Orange$lnConfirmed, col='darkorange')

lines(LA1.pred$Date, LA1.pred$fit, col='lightblue', lwd=1.4)
lines(LA2.pred$Date, LA2.pred$fit, col='blue', lwd=1.4)
lines(LA3.pred$Date, LA3.pred$fit, col='darkblue', lwd=1.4)

lines(Orange1.pred$Date, Orange1.pred$fit, col='orange', lwd=1.4)
lines(Orange2.pred$Date, Orange2.pred$fit, col='darkorange', lwd=1.4)
lines(Orange3.pred$Date, Orange3.pred$fit, col='darkorange4', lwd=1.4)

text(as.Date("2020-04-01"), 9.7,paste("Current LA Doubling Rate =", LA3.dr), col='darkblue')
text(as.Date("2020-04-14"), 5.4, paste("Current Orange Doubling Rate =", Orange3.dr), col='darkorange4')

```

As we might guess, the rates vary dramatically between counties -- and more interesting, the impact of social distancing has differential impacts based on compliance and county characteristics. I suspect housing density, income, and exposure rates all play a role. Figuring out how to model that is going to be a challenge with such limited data.

## Hospital Bed Shortages

```{r beds, echo=FALSE}
totalbeds=2.1/1000*10.6e6
beds= totalbeds*.50 # occupancy rate
beds=beds*2 # with navy, convention center
```

We have about 2.1 beds per 1,000 people in the state of California. With approximately 10.6 million residents in LA County, there should be approximately `r (totalbeds)` beds in the county. However, they are not sitting around empty. In normal times there is a 50% occupancy rate. There is no reason to believe this is a normal time, and to guess if that rate is higher or lower is anyone's guess. Nevertheless, for the sake exploring the implications, let's estimate the number of empty beds in LA County based on these assumptions and `r (beds)` beds exists in LA County. However, the city has been working hard to ``create'' additional beds -- converting convention centers, US Navy Hospital Ship, etc. I wouldn't be surprised if we had doubled the number of beds to handle the sitution, thus, `r (beds)`. Let's see how this lands on our growth rates.

```{r, echo=FALSE, results='hide'}
BedsByConfirmed = beds/.2; BedsByConfirmed
beddate = (log(BedsByConfirmed)-coef(LA3.lm)[1])/coef(LA3.lm)[2]; beddate
as.numeric(as.Date('1970-01-01')) # where is the origin

Bdate = as.Date(beddate, origin = "1970-01-01"); Bdate
Bdate2 = format(Bdate,"%m/%d"); Bdate2
```

Based on some WHO estimates, let's assume about 20% of the confirmed cases need hospital beds, then LA County will run out of beds on `r Bdate2`. However, what this model is missing is the category of 'recovered'. It's safe to say that after 20 days of being hospitalized patients will either be in recovery (and can go home) or have died. At this point, I haven't included this in the model, thus this prediction is a worst case scenario. 

According to the LA Times, there are roughly 200 ICU beds in the county. I think about 5% of these who get sick need access to ICU. I suspect this is going to be an even more serious issue, but I haven't had time to model this yet. 

```{r bedshortage, echo=FALSE, cap.fig='Predicted Date the Available Hospital Beds will be Exceeded'}
# Projected values
options(scipen=5)

plot(Confirmed ~ Date, data=LA, las=1, yaxt='none', ylab="Confirmed Cases", pch=20, log="y",
xlim=c(as.Date('2020-03-01'), as.Date('2020-07-29')), ylim=c(0.1, 300000))

axis(2, at=c(1, 10, 30, 100, 300, 1000, 3000, 10000, 30000, 100000, 300000), labels = c("1", "10", "30", "100", "300", "1,000", "3,000", "10,000", "30,000", "100,000", "300,000"), las=2)
#axis(2, seq(0,1.5,.5),las=1, cex.axis=1)


new_dates=tibble(Date= ymd('2020-04-03', temp))
  

wind1 <- round(c(nrow(LA1.pred)*.33, nrow(LA1.pred)*.66),0)
wind2 <- round(c(nrow(LA2.pred)*.33, nrow(LA2.pred)*.66),0)
wind3 <- round(c(nrow(LA3.pred)*.33, nrow(LA3.pred)*.66),0)

movingpred <- rbind(LA1.pred[wind1,],LA2.pred[wind2,],LA3.pred[wind3,]) 

points(movingpred$Date, exp(movingpred$fit), pch=20, col='red')
# Error Bars

arrows(x0=movingpred$Date, y0=exp(movingpred$fit), y1=exp(movingpred$upr), code=2, col="red", lwd=.5, angle = 90, length=.03)
arrows(x0=movingpred$Date, y0=exp(movingpred$fit), y1=exp(movingpred$lwr), code=2, col="red", lwd=.5, angle = 90, length=.03)

arrows(x0= as.Date(Bdate), y0 = 300000, y1= BedsByConfirmed, length = 0.15, angle = 20, code = 2, col = 'darkgoldenrod', lwd = 1)
text(as.Date(Bdate), 300000, paste("County Bed Capacity Exceeded", Bdate2), pos=2, col='darkgoldenrod')

```

With the assumption that 20% of the confirmed cases need to be hospitalized, the county should have adequate resources to meeting the demand -- as a direct result of the social distancing policies. This model is highly simplified and by modeling doubling rate as a constant, it account for the impacts of social distancing in crude ways, by modeling doubling rates within specified windows.



So, next I'll address the non-linear growth component of Covid-19 and figure out how to consider this as a function of an SIR model.

# Fitting a Simple Epidemic Model Using Maximum Likelihood

A more sophisticated approach use the advantages of maximum likelihood estimation, which include 

- consistent approach to statistical inference; 
- minimum variance unbiased estimation; and
- asymptotic normality of MLEs. 

Likelihood is also an essential component of Bayesian estimation, which allows for many more options in the analysis process. 

For this section, I have effectively modified code used by James Jones (http://web.stanford.edu/class/earthsys214/notes/fit.html) after a week of trying numerous deadends. 

The approach is based on the SIR epidemiology model, which is a compartment model that categorizes subjects into "susceptable", "infectious" and "recovered". It's probably the simplist model, which for me is a good place to start! 

The SIR is defined by ordinary differential equations (ODE). In R, we can capitalize on the deSolve package to define and solve these equations. 

The function for this in R is defined below: 

```{r}
### SIR Approach
sir <- function(t,x,parms){
  S <- x[1]
  I <- x[2]
  R <- x[3]
  with(as.list(parms),
       {
         dS <- -beta*S*I
         dI <- beta*S*I # - nu*I
         dR <- nu*I
         res <- c(dS,dI,dR)
         list(res)
       })
}
```
Jones addes some handy code to track the total number of infections over the course of each model run. Nice!

Next, Jones define functions to to calculate the maximum likelihood estimators (MLEs) using Bolker's R package, bbmle. I still need to do some homework to figure out what's going on here, but for now, I got it working!

## Notes on likelihood Model

For this model, the assumption is being used that the measured values of confirmed cases have errors that are normally distributed. Jonese recommneds a process-error model might use a Poisson or negative binomial, but I can't get these to work yet.

Optimization routine require some constraints -- some common sense boundaries are useful here, e.g. negative transmission rate are non-negative. Confirmed cases must have been infected first -- thus, total confirmed > total infected. 

Now, here's a section that I don't understand at all! 

According to Jones, although SIR model are not really probabilities, they are typically less than one and greater than zero -- this have ranges like probabilities. Thus, we can constrain the rates between 0 and 1. Apparently, we can use a logit-transformation (using the quantile function for the logistic distribution, qlogis()) to ensure that the rates are between 0 and 1. 

Thus, Jones' code passes "the optimization function logit-transformed values which the function then back-transforms using plogis()." Thankfully, his code is available to do this, because I would never be able to figure out what that means from scratch. Furthermore, he notes that log-transformed population size and number of initial infections are also passed through a function back-transforms them. God knows what this means -- but it does pretty well in terms of modeling the data!

```{r, message=FALSE, results='hide'}
### Covid19 application

# likelihood function
sirLL <- function(lbeta, lnu, logN, logI0) {
  parms <- c(beta=plogis(lbeta), nu=plogis(lnu))
  x0 <- c(S=exp(logN), I=exp(logI0), R=0)
  out <- ode(y=x0, t, sir, parms)
  SD <- sqrt(sum( (C-out[,3])^2)/length(t) )
  -sum(dnorm(C, mean=out[,3], sd=SD, log=TRUE))
}

```

## Define Data Sets and Run Models

```{r runmodels}
# Unfortunately, the model is models mortality
record = length(LA$Confirmed)-0
C = LA$Confirmed[1:record]
D = LA$Deaths[51:record]
t = 1:length(C); days=max(t); days

# minimize negative-log-likelihood
covid.fit <- mle2(sirLL, 
            start=list(lbeta=qlogis(1e-5), 
                       lnu=qlogis(.2), 
                       logN=log(1e8), logI0=log(1) ),  
            method="Nelder-Mead",
            control=list(maxit=1E5,trace=0),
            trace=FALSE)

record = length(Orange$Confirmed)-0
C = Orange$Confirmed[1:record]
D = Orange$Deaths[51:record]
t = 1:length(C); days=max(t); days

# minimize negative-log-likelihood
covid.fit.orange <- mle2(sirLL, 
            start=list(lbeta=qlogis(1e-5), 
                       lnu=qlogis(.2), 
                       logN=log(1e8), logI0=log(1) ),  
            method="Nelder-Mead",
            control=list(maxit=1E5,trace=0),
            trace=FALSE)

record = length(SD$Confirmed)-0
C = SD$Confirmed[1:record]
D = SD$Deaths[51:record]
t = 1:length(C); days=max(t); days

# minimize negative-log-likelihood
covid.fit.SD <- mle2(sirLL, 
            start=list(lbeta=qlogis(1e-5), 
                       lnu=qlogis(.2), 
                       logN=log(1e8), logI0=log(1) ),  
            method="Nelder-Mead",
            control=list(maxit=1E5,trace=0),
            trace=FALSE)

```

```{r, echo=FALSE, results='hide', eval=FALSE}
# number of days to predict t.p 
t_p = 160

tmp=summary(covid.fit);tmp@coef[,2]
theta <- as.numeric(c(
  plogis(coef(covid.fit)[1:2]),
  exp(coef(covid.fit)[3:4]))); theta
theta.upr <- as.numeric(c(
  plogis(coef(covid.fit)[1:2] + 1.96* tmp@coef[1:2,2]),
  exp(coef(covid.fit)[3:4]+ 1.96*tmp@coef[3:4,2]))); theta.upr
theta.lwr <- as.numeric(c(
  plogis(coef(covid.fit)[1:2] - 1.96*tmp@coef[1:2,2]),
  exp(coef(covid.fit)[3:4]- 1.96*tmp@coef[3:4,2]))); theta.lwr
parms <- c(beta=theta[1], nu = theta[2])
parms.upr <- c(beta=theta.upr[1], nu = theta.upr[2])
parms.lwr <- c(beta=theta.lwr[1], nu = theta.lwr[2])
times <- seq(0,t_p,0.1)

x0 <- c(theta[3],theta[4],0)
x0.upr <- c(theta.upr[3],theta.upr[4],0)
x0.lwr <- c(theta.lwr[3],theta.lwr[4],0)
stateMatrix1 <- ode(y=x0, times, sir, parms)
stateMatrix1.upr <- ode(y=x0.upr, times, sir, parms.upr)
stateMatrix1.lwr <- ode(y=x0.lwr, times, sir, parms.lwr)
colnames(stateMatrix1) <- c("time","S","I","R")
colnames(stateMatrix1.upr) <- c("time","S","I","R")
colnames(stateMatrix1.lwr) <- c("time","S","I","R")
plot(stateMatrix1[,"time"], stateMatrix1[,"I"], type="l", lwd=2, 
     xaxs="i", xlab="Days", ylab="Confirmed Cases", las=1, ylim=c(0,max(stateMatrix1.upr[,"I"])))
lines(stateMatrix1.upr[,"time"], stateMatrix1.upr[,"I"], col='grey')
lines(stateMatrix1.lwr[,"time"], stateMatrix1.lwr[,"I"], col='grey')


covid.fit@vcov

LApeak = round(x0[1], -2)

# adding trace to see progress...
#fit2 <- mle2(sirLL, start=as.list(coef(fit)), fixed=list(logN=coef(fit)[3],  logI0=coef(fit)[4]), method="Nelder-Mead",control=list(maxit=1E5,trace=2),trace=TRUE)

## Adding Orange County
record = length(Orange$Confirmed)-0
C = Orange$Confirmed[1:record]
D = Orange$Deaths[51:record]
t = 1:length(C); days=max(t); days
tmp=summary(covid.fit.orange);tmp@coef[,2]
theta <- as.numeric(c(
  plogis(coef(covid.fit.orange)[1:2]),
  exp(coef(covid.fit.orange)[3:4]))); theta
theta.upr <- as.numeric(c(
  plogis(coef(covid.fit.orange)[1:2] + 1.96* tmp@coef[1:2,2]),
  exp(coef(covid.fit.orange)[3:4]+ 1.96*tmp@coef[3:4,2]))); theta.upr
theta.lwr <- as.numeric(c(
  plogis(coef(covid.fit.orange)[1:2] - 1.96*tmp@coef[1:2,2]),
  exp(coef(covid.fit.orange)[3:4]- 1.96*tmp@coef[3:4,2]))); theta.lwr
parms <- c(beta=theta[1], nu = theta[2])
parms.upr <- c(beta=theta.upr[1], nu = theta.upr[2])
parms.lwr <- c(beta=theta.lwr[1], nu = theta.lwr[2])
times <- seq(0, t_p, 0.1)

x0 <- c(theta[3],theta[4],0)
x0.upr <- c(theta.upr[3],theta.upr[4],0)
x0.lwr <- c(theta.lwr[3],theta.lwr[4],0)
stateMatrix1 <- ode(y=x0, times, sir, parms)
stateMatrix1.upr <- ode(y=x0.upr, times, sir, parms.upr)
stateMatrix1.lwr <- ode(y=x0.lwr, times, sir, parms.lwr)
colnames(stateMatrix1) <- c("time","S","I","R")
colnames(stateMatrix1.upr) <- c("time","S","I","R")
colnames(stateMatrix1.lwr) <- c("time","S","I","R")
lines(stateMatrix1[,"time"], stateMatrix1[,"I"], col="darkorange4")
lines(stateMatrix1.upr[,"time"], stateMatrix1.upr[,"I"], col='grey')
lines(stateMatrix1.lwr[,"time"], stateMatrix1.lwr[,"I"], col='grey')
points(t, C, pch=16, col="darkorange4", cex=.5)

Orangepeak = round(x0[1], -2)

## Adding SD County
record = length(SD$Confirmed)-0
C = SD$Confirmed[1:record]
D = SD$Deaths[51:record]
t = 1:length(C); days=max(t); days
tmp=summary(covid.fit.SD);tmp@coef[,2]
theta <- as.numeric(c(
  plogis(coef(covid.fit.SD)[1:2]),
  exp(coef(covid.fit.SD)[3:4]))); theta
theta.upr <- as.numeric(c(
  plogis(coef(covid.fit.SD)[1:2] + 1.96* tmp@coef[1:2,2]),
  exp(coef(covid.fit.SD)[3:4]+ 1.96*tmp@coef[3:4,2]))); theta.upr
theta.lwr <- as.numeric(c(
  plogis(coef(covid.fit.SD)[1:2] - 1.96*tmp@coef[1:2,2]),
  exp(coef(covid.fit.SD)[3:4]- 1.96*tmp@coef[3:4,2]))); theta.lwr
parms <- c(beta=theta[1], nu = theta[2])
parms.upr <- c(beta=theta.upr[1], nu = theta.upr[2])
parms.lwr <- c(beta=theta.lwr[1], nu = theta.lwr[2])
times <- seq(0, t_p, 0.1)

x0 <- c(theta[3],theta[4],0)
x0.upr <- c(theta.upr[3],theta.upr[4],0)
x0.lwr <- c(theta.lwr[3],theta.lwr[4],0)
stateMatrix1 <- ode(y=x0, times, sir, parms)
stateMatrix1.upr <- ode(y=x0.upr, times, sir, parms.upr)
stateMatrix1.lwr <- ode(y=x0.lwr, times, sir, parms.lwr)
colnames(stateMatrix1) <- c("time","S","I","R")
colnames(stateMatrix1.upr) <- c("time","S","I","R")
colnames(stateMatrix1.lwr) <- c("time","S","I","R")
lines(stateMatrix1[,"time"], stateMatrix1[,"I"], col="darkorange4")
lines(stateMatrix1.upr[,"time"], stateMatrix1.upr[,"I"], col='grey')
lines(stateMatrix1.lwr[,"time"], stateMatrix1.lwr[,"I"], col='grey')
points(t, C, pch=16, col="purple", cex=.5)

SDpeak = round(x0[1], -2)
text(10, SDpeak, "San Diego")
text(90, LApeak, "Los Angeles")
text(40, Orangepeak, "Orange")

record = length(LA$Confirmed)-0
C = LA$Confirmed[1:record]
D = LA$Deaths[51:record]
t = 1:length(C); days=max(t); days
points(t, C, pch=16, col="red", cex=.5)

arrows(x0 = max(t), y0=max(C),
       x1 = max(t)+30, y1 = max(C),
       length = 0.05, angle = 90, code = 3, 
       col = 'orange', lwd = 1)
arrows(x0 = max(t)+30, y0=max(C),
       x1 = max(t)+30, y1 = LApeak,
       length = 0.05, angle = 90, code = 3, 
       col = 'orange', lwd = 1)
```

The estimated peak for covid-19 in LA will be r LApeak and r Orangepeak for Los Angeles and Orange County, respectively, as modeled on `r Sys.Date()`, using data through the `r max(as.Date(Daily$Date), na.rm=TRUE)`. 

### Model Sensitivity to Data Timestep

The simple model appeared reliable until we start reaching until middle April. The model suggested an assymatote of around 10,000 as a plateau in the 3rd week of April. However, as we approached the date, the plataeu starting moving just out of reach, until the model became completely ineffective. Thus, it becomes a completely lame precictive tool. 

The accuracy and error estimates are preform poorly, the model needs to be refined in a dramatic way. Currently, the model might need to be estimated as a logistic of the estimates!

```{r, echo=FALSE, results='hide', eval=FALSE}

LA.estimates.dates = seq(as.Date("2020-04-04"), max(LA$Date), 1); LA.estimates.dates

LA.estimates = c(
# April 4: 9,100; April 5: 10,100; April 6: 9,800
9100, 10100, 9800,
# April 7: 9,800; April 8: 10,200; April 9: 10,200
9800, 10200, 10200,
# April 10: 10,400; April 11: 10,500; April 12, 10,600
10400, 10500, 10600,
# April 13, 10,600; April 14, 10,900; April 15, 11,200
10600, 10900, 11200,
# April 16, 11,500; April 17, 11,900; April 18, 12,400
11500, 11900, 12400,
# April 19, 12,800
12800,
# April 20, 13,800* Backlog of tests reported to DHS
13800,
# April 21, 15,300 The model is now completely failing
15300,
# April 22, 17,600 ugh!!! This sucks!
17600,
# April 23, 20500, now we have a new model that actually predict growth again for atleast 20 more days. Ugh!
20500,
# April 24, 23,700; April 25, 26,200; April 26, 27,500
23700, 26000, 27500,
# April 27, 28,600; 
28600, 29100, 30700, 31800, 
# May 1 
33000, 33700,34400, 34700, 35800, 
# May 6 
36800, 37700, 38400, 39600, 39600, 
#May 11
39900, 40300, 41100, 41900, 42700,
# May 16
43600, 44200, 44900, 45600, 46500,
# May 21
47500, 48600, 49700, 50800, 51900, 
# May 26
53400, 54800, 56200, 57900)

if(length(LA.estimates)+1==length(LA.estimates.dates)){
  LA.estimates=c(LA.estimates, LApeak)
}

estimates = data.frame(Date = LA.estimates.dates, Estimates = LA.estimates) 

plot(Estimates/1000 ~ Date, data=estimates, main="Predicted Confirmed Cases by Date", las=1, col="red", pch=15,
     ylab="Confirmed (Thousands)")


```


 
# Mortality 

```{r, echo=FALSE}
CA$County <- CA$Admin2
densities = read.csv("/home/CAMPUS/mwl04747/github/test/Epidemiology/County.Densities.csv", header=FALSE)
names(densities)=c("County", "Density")
CA_densities = merge(CA, densities)
CA_densities$Death.d = as.numeric(CA_densities$Deaths) / CA_densities$Density
populations = read.csv("/home/CAMPUS/mwl04747/github/test/Epidemiology/CA_County_Population.csv")
CA_pop = merge(CA_densities, populations)
CA_pop$Death.r <- CA_pop$Deaths/CA_pop$Population*10000
CA_pop$Confirmed.r <- CA_pop$Confirmed/CA_pop$Population*10000

```


## Selected Counties

There are 58 counties in CA, too many to view at once. So, I selected a few SoCal, BayArea, and rural counities to compare.

```{r}
Selected = subset(CA_pop, subset=(County=="Los Angeles" | County=="Orange" | County=="San Bernardino" | County=="San Diego" | County=="Santa Cruz" | County=="San Francisco" | County=="Santa Clara" | County=="Del Norte" | County=="Monterey" | County=="Mono" | County=="San Luis Obispo"))

#plot(Death.r ~ as.Date(Date), data=Selected, las=1, pch=20, col = 'red')
```

## Confirmed Cases and Deaths by County
```{r}

C <- ggplot(Selected, aes(x = Date, y = Confirmed.r, color = County)) + geom_line() + geom_point(aes(shape=County), size=2) + scale_shape_manual(values=c(3, 16, 17, 20, 3, 16, 17, 20, 3, 16, 17)) + labs(y = "Confirmed/10,000")

D.r <- ggplot(Selected, aes(x = Date, y = Death.r, color = County)) + geom_line() + geom_point(aes(shape=County), size=2) + scale_shape_manual(values=c(3, 16, 17, 20, 3, 16, 17, 20, 3, 16, 17)) + labs(y = "Deaths/10,000")

library(ggpubr)

figure <- ggarrange(C, D.r,
    labels = c("A", "B"),
    ncol = 2, nrow = 1,
    common.legend = TRUE, legend = "bottom")
figure
```

## Los Angeles and San Francisco are outliers

What is an outlier?  Samples that don't seem to fall into the expected distribution pattern. 

```{r echo=FALSE}
CA_dates = subset(CA_pop, subset=(Date==max(Daily$Date, na.rm=T) | Date=="2020-04-01" | Date=="2020-05-01"), select=c(Date, County, Density, Deaths, Death.r, Population, Confirmed))
CA_dates$pop = CA_dates$Population/1000000
CA_dates$con = CA_=CA_dates$Confirmed/1000
CA_dates$'% Confirmed per capita' = CA_dates$Confirmed/CA_dates$Population*100
CA_maxdate = subset(CA_dates, subset=(Date==max(CA_dates$Date, na.rm=T)))
```

```{r echo=FALSE}
par(mfrow=c(1, 2), las=1)
plot(con ~ pop, CA_maxdate, cex=.5, xlab="Population (Millions)",
ylab="Confirmed (x 1,000)")
points(con ~ pop, CA_maxdate[CA_maxdate$County == "San Francisco",], col="green", pch=20)
points(con ~ pop, CA_maxdate[CA_maxdate$County == "Los Angeles",], col="purple", pch=20)
points(con ~ pop, CA_maxdate[CA_maxdate$County == "Orange",], col="orange", pch=20)
points(con ~ pop, CA_maxdate[CA_maxdate$County == "San Luis Obispo",], col="blue", pch=20)
points(con ~ pop, CA_maxdate[CA_maxdate$County == "Yolo",], col="darkred", pch=20)
points(con ~ pop, CA_maxdate[CA_maxdate$County == "Tulare",], col="red", pch=20)

legend(1, 30, legend=c("San Francisco", "Los Angeles", "Orange", "San Luis Obispo", "Yolo", "Tulare"), pch=20, col=c("green", "purple", "orange", "blue", "darkred", "red"), cex=.7)

plot(Death.r ~ Density, CA_maxdate, cex=.5, ylab="Death Rate (by 10,000)")
points(Death.r ~ Density, CA_maxdate[CA_maxdate$County == "San Francisco",], col="green", pch=20)
points(Death.r ~ Density, CA_maxdate[CA_maxdate$County == "Los Angeles",], col="purple", pch=20)
points(Death.r ~ Density, CA_maxdate[CA_maxdate$County == "Orange",], col="orange", pch=20)
points(Death.r ~ Density, CA_maxdate[CA_maxdate$County == "San Luis Obispo",], col="blue", pch=20)
points(Death.r ~ Density, CA_maxdate[CA_maxdate$County == "Yolo",], col="darkred", pch=20)
points(Death.r ~ Density, CA_maxdate[CA_maxdate$County == "Tulare",], col="red", pch=20)
```

Los Angeles has by far the most cases -- but certainly along the same lines that it also has a very large population relative to other counties. On a per capita basis, the numbers demonstrate the impact is impacting southern California much more than other parts of the state.

```{r echo=FALSE, results='hide'}
CA_april = subset(CA_dates, subset=Date=="2020-04-01")[,c(2,7,10)]
names(CA_april) = c("County", "Confirmed--April", "% Confirmed per capita--April")
CA_may = subset(CA_dates, subset=Date=="2020-05-01")[,c(2,7,10)]
names(CA_may) = c("County", "Confirmed--May", "% Confirmed per capita--May")
CA_monthly = merge(CA_may, CA_april, by="County")
CA_monthly = merge(CA_maxdate, CA_monthly, by="County")

library(xtable)

confirmed.tab = CA_monthly[order(CA_monthly$'% Confirmed per capita', decreasing=TRUE),]; confirmed.tab  = confirmed.tab[1:8, c(1,7,11,13,10, 12,14)]

CA_april = subset(CA_dates, subset=Date=="2020-04-01")[,c(2, 4, 5)]
names(CA_april) = c("County", "Deaths--April", "Death Rate--April")
CA_may = subset(CA_dates, subset=Date=="2020-05-01")[,c(2,4,5)]
names(CA_may) = c("County", "Deaths--May", "Death Rate--May")
CA_monthly = merge(CA_may, CA_april, by="County")
CA_monthly = merge(CA_maxdate, CA_monthly, by="County")



deaths.tab = CA_monthly[order(CA_monthly$Death.r, decreasing=TRUE),]; deaths.tab  = deaths.tab[1:8, c(1, 4, 11, 13, 5, 12, 14)]
#confirmed.tab 
```



```{r, echo=FALSE, results="asis"}
print(xtable(confirmed.tab), type='html', include.rownames=FALSE)
```

This table demonstrates how much Covid-19 is impacting both urban (San Francisco, LA) and rural counties (Tulare, Kings). I wonder how income plays a role?

San Francisco, has a very high density -- but it's mortality rates are suprisingly low. On the other hand, LA has a mortality rate very high -- even relative to it's density. 

But the death rates include some surprises -- Tulare County (`r round(CA_maxdate$Death.r[CA_maxdate$County=="Tulare"], 2)`) and Yolo County (`r round(CA_maxdate$Death.r[CA_maxdate$County=="Yolo"], 2)`). These are surpringly high!

```{r, echo=FALSE, results="asis"}
print(xtable(deaths.tab), type='html', include.rownames=FALSE)
```


## Excess Death Rate by County

The CDPHS puts out a report that catalogued historic deaths by county in 2019. Based on this, but I need real time deaths by county by day or week, instead, I have weekly deaths for the entire state. I will use county averages from the last two years to see k 

```{r}
cdc.import = read.csv("https://data.cdc.gov/api/views/muzy-jte6/rows.csv?accessType=DOWNLOAD")

names(cdc.import)=c("State", "Year", "Week", "EndDate", "Deaths.all")

cdc.CA = subset(cdc.import, subset=c(State=="California" & Year==2020), select=c("State", "Year", "Week", "EndDate", "Deaths.all")); str(cdc.CA)
cdc.CA <- droplevels(cdc.CA)

CA_deaths = read.csv("/home/CAMPUS/mwl04747/github/test/Epidemiology/CA_County_Deaths.csv")

str(CA_deaths)
CA_deaths$Weekly = CA_deaths$Ave_Deaths/52

names(CA_deaths)

Deaths.Weekly <- aggregate(Deaths~week(Date)+ County, data = CA, sum)

names(Deaths.Weekly)<- c("Week", "County", "Covid.Deaths")

CA.deaths = merge(CA_deaths, Deaths.Weekly, by="County")
CA.deaths$Excess = (CA.deaths$Weekly+CA.deaths$Covid.Deaths)/CA.deaths$Weekly*100

plot(Excess~Week, CA.deaths)

#ggplot(CA.deaths, aes(x = Week, y = Deaths, color = County)) + geom_line()

#+ geom_point(aes(shape=County), size=2) + scale_shape_manual(values=c(3, 16, 17, 20, 3, 16, 17, 20, 3, 16, 17)) + labs(y = "Deaths/10,000")



```



# When do we make a policy decision?

Let's say we might want to say that after 5000 cases, we should have known that social distancing was key--- what date should we have made a decision?

What I did is take Jones' code and then subset the data after the first indiciation that there cases in LA -- starting from when the cases went from 1 to 7 as Day 1. Then I looked at model predictions for day 7, 10, 11, 12, 14. 

```{r, echo=FALSE, results='hide', eval=FALSE}
dayzero = 32 # 1 confirmed before jumps to 7
# Day 0
C = LA$Confirmed[1:sum(dayzero,0)]
t = 1:length(C); days=max(t); days
covid.fit <- mle2(sirLL, 
            start=list(lbeta=qlogis(1e-5), 
                       lnu=qlogis(.2), 
                       logN=log(1e6), logI0=log(1) ),  
            method="Nelder-Mead",
            control=list(maxit=1E5,trace=0),
            trace=FALSE)
theta <- as.numeric(c(plogis(coef(covid.fit)[1:2]),
                      exp(coef(covid.fit)[3:4]))); theta
parms <- c(beta=theta[1], nu = theta[2])
x0 <- c(theta[3],theta[4],0)
stateMatrix.0 <- ode(y=x0, times, sir, parms)
colnames(stateMatrix.0) <- c("time","S","I","R")

## Day 3
C = LA$Confirmed[1:sum(dayzero+2)]
t = 1:length(C); days=max(t); days
covid.fit <- mle2(sirLL, 
            start=list(lbeta=qlogis(1e-5), 
                       lnu=qlogis(.2), 
                       logN=log(1e6), logI0=log(1) ),  
            method="Nelder-Mead",
            control=list(maxit=1E5,trace=0),
            trace=FALSE)
theta <- as.numeric(c(plogis(coef(covid.fit)[1:2]),
                      exp(coef(covid.fit)[3:4]))); theta
parms <- c(beta=theta[1], nu = theta[2])
x0 <- c(theta[3],theta[4],0)
stateMatrix.3 <- ode(y=x0, times, sir, parms)
colnames(stateMatrix.3) <- c("time","S","I","R")

## Day 5
C = LA$Confirmed[1:sum(dayzero,4)]
t = 1:length(C); days=max(t); days
covid.fit <- mle2(sirLL, 
            start=list(lbeta=qlogis(1e-5), 
                       lnu=qlogis(.2), 
                       logN=log(1e6), logI0=log(1) ),  
            method="Nelder-Mead",
            control=list(maxit=1E5,trace=0),
            trace=FALSE)
theta <- as.numeric(c(plogis(coef(covid.fit)[1:2]),
                      exp(coef(covid.fit)[3:4]))); theta
parms <- c(beta=theta[1], nu = theta[2])
x0 <- c(theta[3],theta[4],0)
stateMatrix.5 <- ode(y=x0, times, sir, parms)
colnames(stateMatrix.5) <- c("time","S","I","R")

## Day 5 (From Day 0)
C = LA$Confirmed[dayzero:sum(dayzero,5)]
t = 1:length(C); days=max(t); days
covid.fit <- mle2(sirLL, 
            start=list(lbeta=qlogis(1e-5), 
                       lnu=qlogis(.2), 
                       logN=log(1e6), logI0=log(1) ),  
            method="Nelder-Mead",
            control=list(maxit=1E5,trace=0),
            trace=FALSE)
theta <- as.numeric(c(plogis(coef(covid.fit)[1:2]),
                      exp(coef(covid.fit)[3:4]))); theta
parms <- c(beta=theta[1], nu = theta[2])
x0 <- c(theta[3],theta[4],0)
stateMatrix.5.0 <- ode(y=x0, times, sir, parms)
colnames(stateMatrix.5.0) <- c("time","S","I","R")

## Day 10
C = LA$Confirmed[dayzero:sum(dayzero, 9)]
t = 1:length(C); days=max(t); days
covid.fit <- mle2(sirLL, 
            start=list(lbeta=qlogis(1e-5), 
                       lnu=qlogis(.2), 
                       logN=log(1e6), logI0=log(1) ),  
            method="Nelder-Mead",
            control=list(maxit=1E5,trace=0),
            trace=FALSE)
theta <- as.numeric(c(plogis(coef(covid.fit)[1:2]),
                      exp(coef(covid.fit)[3:4]))); theta
parms <- c(beta=theta[1], nu = theta[2])
x0 <- c(theta[3],theta[4],0)
stateMatrix.10.0 <- ode(y=x0, times, sir, parms)
colnames(stateMatrix.10.0) <- c("time","S","I","R")

#########  Figure  #################
C = LA$Confirmed[1:length(LA$Confirmed)]
t = 1:length(C); days=max(t); days
covid.fit <- mle2(sirLL, 
            start=list(lbeta=qlogis(1e-5), 
                       lnu=qlogis(.2), 
                       logN=log(1e6), logI0=log(1) ),  
            method="Nelder-Mead",
            control=list(maxit=1E5,trace=0),
            trace=FALSE)
theta <- as.numeric(c(plogis(coef(covid.fit)[1:2]),
exp(coef(covid.fit)[3:4]))); theta
parms <- c(beta=theta[1], nu = theta[2])
x0 <- c(theta[3],theta[4],0)
stateMatrix1 <- ode(y=x0, times, sir, parms)
colnames(stateMatrix1) <- c("time","S","I","R")

theta <- as.numeric(c(
  plogis(coef(covid.fit)[1:2]),
  exp(coef(covid.fit)[3:4]))); theta
theta.upr <- as.numeric(c(
  plogis(coef(covid.fit)[1:2] + 1.96* tmp@coef[1:2,2]),
  exp(coef(covid.fit)[3:4]+ 1.96*tmp@coef[3:4,2]))); theta.upr
theta.lwr <- as.numeric(c(
  plogis(coef(covid.fit)[1:2] - 1.96*tmp@coef[1:2,2]),
  exp(coef(covid.fit)[3:4]- 1.96*tmp@coef[3:4,2]))); theta.lwr
parms <- c(beta=theta[1], nu = theta[2])
parms.upr <- c(beta=theta.upr[1], nu = theta.upr[2])
parms.lwr <- c(beta=theta.lwr[1], nu = theta.lwr[2])

times <- seq(0,100,0.1)
plot(stateMatrix1[,"time"], stateMatrix1[,"I"], type="l", lwd=2, 
     xaxs="i", xlab="Days", ylab="Confirmed Cases", las=1, ylim=c(0, 1000), xlim=c(0,70))
lines(stateMatrix.0[,"time"], stateMatrix.0[,"I"], lwd=2, col="darkgreen")
lines(stateMatrix.3[,"time"], stateMatrix.3[,"I"], lwd=2, col="blue")
lines(stateMatrix.5[,"time"], stateMatrix.5[,"I"], lwd=2, col="purple")
lines(stateMatrix.5.0[,"time"]+dayzero-1, stateMatrix.5.0[,"I"], lwd=2, col="orange")
lines(stateMatrix.10.0[,"time"]+dayzero-1, stateMatrix.10.0[,"I"], lwd=2, col="darkred")
points(t, C, pch=16, col="red", cex=.75)

dayorder = 15 + dayzero
arrows(x0= dayorder, y0 = 400, y1= C[dayorder], length = 0.15, angle = 20, code = 2, col = 'darkgoldenrod', lwd = 1)
text(dayorder, 400, paste("County 'Safe-at-Home Order' Day =", dayorder-dayzero), pos=2, col='darkgoldenrod')

arrows(x0= dayzero, y0 = 200, y1= C[dayzero], length = 0.15, angle = 20, code = 2, col = 'orange', lwd = 1)
text(dayzero, 200, "Day Zero", pos=2, col='orange')

legend(x=2, y=1000, legend=c("Full Time Series Modelled", "Day 0 (March 3rd)", "Day 3 (March 6th)", "Day 5 (March 8th)", "Day Zero- Day 5 (March 8th)", "Day Zero - Day 10 March 13th)", "Observed Cases"), col=c("black", "darkgreen", "blue", "purple", "orange", "darkred", "red"), lty=c(1,1,1,1,1,1,0), pch=c(NA, NA, NA, NA, NA, NA, 20), cex=.85, bty='n')
```

The first indication that we were going to have a significant increase in cases came on Day 1, but how can officials call for social distancing with only 7 cased in the county?  The model show dramatic increases as soon as this occurs, but the model is wildly alarming and would have predicted a dramatic increase that was not seen -- instead when the model is uses only data from dayzero, the model aligns with the data well even within 5 days of the increase in confirmed cases. 

Lesson: Models that have been used to model an epidemic can easily be applied for decision making once confirmed cases begin some growth, but the time frame must be constrained to avoid including the time before detections were found, otherwise the results are quite inaccurate and might lead to strong pushback on the modeling approaches and failure to take the epidemic seriously. Day 11, March 14th. However, the slope of the curve was far from alarming and suggested decisions didn't have to be made yet. 

Based on this simple models, the 'safe-at-home' order could have come by day 5 of the increase. To be clear, there is a time lag in all policy decisions. For example, during the Cold War there was a lot of effort to ensure that the president could launch a missle attack within a few minutes of an attack. That required dedicated infrastruture and policy to support a rapid response time. Public health officials do not enjoy that level of decisions support, so a 2 or 3 day lag is probably pretty reasonable for Covid-19, so what might have happened had the order come on Day 8?

## How to use data to predict outcomes?

Not sure how to do this, working on it...


```{r, eval=FALSE}
C = LA$Confirmed[dayzero:sum(dayzero,5)]
t = 1:length(C); days=max(t); days
covid.fit <- mle2(sirLL, 
            start=list(lbeta=qlogis(1e-5), 
                       lnu=qlogis(.2), 
                       logN=log(1e6), logI0=log(1) ),  
            method="Nelder-Mead",
            control=list(maxit=1E5,trace=0),
            trace=FALSE)
summary(covid.fit)
theta <- as.numeric(c(plogis(coef(covid.fit)[1:2]),
                      exp(coef(covid.fit)[3:4]))); theta
parms <- c(beta=theta[1], nu = theta[2])
x0 <- c(theta[3],theta[4],0)

times.begin = seq(1, 20, .1) 
stateMatrix.begin <- ode(y=x0, times.begin, sir, parms)
colnames(stateMatrix.begin) <- c("time","S","I","R")

times.end = seq(20.1, 100, .1)
x0 = c(10000, max(stateMatrix.begin[,"I"]), 0); x0
parms[1]=parms[1]/2
stateMatrix.end <- ode(y=x0, times.end, sir, parms)
colnames(stateMatrix.end) <- c("time","S","I","R")
stateMatrix.test = rbind(stateMatrix.begin, stateMatrix.end)

C = LA$Confirmed[1:length(LA$Confirmed)]
t = 1:length(C); days=max(t); days
plot(stateMatrix.test[,"time"], stateMatrix.test[,"I"], type="l", lwd=2, 
     xaxs="i", xlab="Days", ylab="Confirmed Cases", las=1, ylim=c(0, 2000), xlim=c(0,70))
```


However, I am curious about what CDC models were saying at the time. I suspect they are much more sophisticated and these might have sent out warnings much sooner. To be continued. 


# Poisson versus Gaussian -- Not Working.

We can think of the outcomes as a process-error framework. Rather than using a normal model for the number of deaths as measured with error, we model the deaths directly as a Poisson random variable.

## A different approach poisson but doesn't work...
```{r eval=FALSE}
sirLL2 <- function(lbeta, lnu, logN, logI0) {
  parms <- c(beta=plogis(lbeta), nu=plogis(lnu))
  x0 <- c(S=exp(logN), I=exp(logI0), R=0)
  out <- ode(y=x0, t, sir, parms)
  -sum(dpois(C, lambda=out[,4], log=TRUE))
}

fit.pois <- mle2(sirLL2, 
                 start=list(lbeta=qlogis(1e-5), 
                            lnu=qlogis(.2), 
                            logN=log(1e6), logI0=log(1) ),  
                 method="Nelder-Mead",
                 control=list(maxit=1E5,trace=2),
                 trace=TRUE)

summary(fit.pois)

theta2 <- as.numeric(c(plogis(coef(fit.pois)[1:2]),
                       exp(coef(fit.pois)[3:4])) )

parms <- c(beta=theta2[1], nu = theta2[2])
times <- seq(0,30,0.1)
x0 <- c(theta2[3],theta2[4],0)
stateMatrix2 <- ode(y=x0, times, sir, parms))
points(weeks, cumbombay, pch=16, col="red")
colnames(stateMatrix2) <- c("time","S","I","R")
plot(stateMatrix2[,"time"], stateMatrix2[,"R"], type="l", lwd=2, 
     xaxs="i", xlab="Time", ylab="Cumulative Deaths")
lines(stateMatrix1[,"time"], stateMatrix1[,"R"], col=grey(0.85), lwd=2
legend("topleft", c("Poisson", "Gaussian"), lwd=2, col=c("black",grey(0.85)))

```

